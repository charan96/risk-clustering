{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from copy import deepcopy as dc\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(cfg.DATA_DIR, '*/*.risk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for f in files:\n",
    "    with open(f, 'r') as fh:\n",
    "        data.extend(fh.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_grams(doc, n_grams):\n",
    "    doc = doc.lower()    \n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc)\n",
    "    \n",
    "    tokens = [token for token in doc.split(\" \") if token != \"\"]\n",
    "    \n",
    "    ngrams = zip(*[tokens[i:] for i in range(n_grams)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def create_bag_of_words_as_list(docs, n_grams=1):\n",
    "    bag_of_words = set()\n",
    "    doc_word_counter_dict = {}\n",
    "    \n",
    "    for doc_id, doc in enumerate(docs):\n",
    "        if n_grams == 1:\n",
    "            split_doc = doc.split(' ')\n",
    "            doc_word_counter_dict[doc_id] = Counter(split_doc)\n",
    "            bag_of_words.update(split_doc)\n",
    "        else:\n",
    "            n_gram_phrases = generate_n_grams(doc, n_grams=n_grams)\n",
    "            doc_word_counter_dict[doc_id] = Counter(n_gram_phrases)\n",
    "            bag_of_words.update(n_gram_phrases)\n",
    "        \n",
    "    stopwords = sw.words('english')\n",
    "    bag_of_words -= set(stopwords)\n",
    "    \n",
    "    clean_bag_of_words = [word for word in bag_of_words if len(word) > 2]\n",
    "    \n",
    "    return list(clean_bag_of_words), doc_word_counter_dict\n",
    "\n",
    "\n",
    "def create_doc_word_matrix(docs, bag_of_words_dict):\n",
    "    global_doc_word_matrix = {doc_idx: [0] * len(bag_of_words_dict) for doc_idx in range(len(docs))}\n",
    "    \n",
    "    for doc_idx in range(len(docs)):\n",
    "        for word_idx, word in bag_of_words_dict.items():\n",
    "            global_doc_word_matrix[doc_idx][word_idx] += doc_word_ctr_dict[doc_idx][word]\n",
    "            \n",
    "    return global_doc_word_matrix\n",
    "\n",
    "\n",
    "def create_term_frequency_matrix(base_matrix):\n",
    "    tf_matrix = dc(base_matrix)\n",
    "    \n",
    "    for doc_id in tf_matrix:\n",
    "        num_words_sum = sum(tf_matrix[doc_id])\n",
    "        for word_idx, _ in enumerate(tf_matrix[doc_id]):\n",
    "            tf_matrix[doc_id][word_idx] /= num_words_sum\n",
    "            \n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def create_idf_matrix(base_matrix):\n",
    "    idf_matrix = dc(base_matrix)\n",
    "    \n",
    "    for doc_id in idf_matrix:\n",
    "        for word_idx, _ in enumerate(idf_matrix[doc_id]):\n",
    "            docs_with_word = sum([1 for i in range(len(idf_matrix)) if idf_matrix[i][word_idx] > 0])\n",
    "            idf_matrix[doc_id][word_idx] = np.log(len(idf_matrix) / docs_with_word)\n",
    "            \n",
    "    return idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_as_list, doc_word_ctr_dict = create_bag_of_words_as_list(data, n_grams=1)\n",
    "bag_of_words_enumerated_dict = dict(enumerate(bag_of_words_as_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_doc_word_matrix(data, bag_of_words_enumerated_dict)\n",
    "tf_matrix = create_term_frequency_matrix(matrix)\n",
    "idf_matrix = create_idf_matrix(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = np.multiply(np.array(list(tf_matrix.values())), np.array(list(idf_matrix.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
